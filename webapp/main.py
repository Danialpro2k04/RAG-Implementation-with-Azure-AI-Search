from openai import AzureOpenAI
from langchain_community.vectorstores import AzureSearch
from langchain_openai import AzureOpenAIEmbeddings
import os
from dotenv import load_dotenv
from fastapi import FastAPI
from fastapi.responses import RedirectResponse
from pydantic import BaseModel
import logging

# Configure logging
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')

app = FastAPI(
    title="Wine Assistant RAG API",
    description="An API that uses Azure AI Search and Azure OpenAI to answer questions about wine."
)


load_dotenv()
logging.info("Attempting to load environment variables...")

required_vars = [
    "AZURE_OPENAI_ENDPOINT", "AZURE_OPENAI_API_KEY", "AZURE_OPENAI_API_VERSION",
    "EMBEDDING_DEPLOYMENT_NAME", "CHAT_DEPLOYMENT_NAME", "SEARCH_SERVICE_NAME",
    "SEARCH_API_KEY", "SEARCH_INDEX_NAME"
]
missing_vars = [var for var in required_vars if not os.getenv(var)]

if missing_vars:
    raise ValueError(
        f"Missing required environment variables: {', '.join(missing_vars)}. "
        "Please check your .env file."
    )
else:
    logging.info("All required environment variables are loaded successfully.")


# Client for Chat Completions
llm_client = AzureOpenAI(
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
)
logging.info("AzureOpenAI client for chat configured.")

# Client for Embeddings
embeddings = AzureOpenAIEmbeddings(
    azure_deployment=os.getenv("EMBEDDING_DEPLOYMENT_NAME"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
)
logging.info("AzureOpenAIEmbeddings client configured.")

# Connection to Azure AI Search Vector Store
vector_store = AzureSearch(
    azure_search_endpoint=os.getenv("SEARCH_SERVICE_NAME"),
    azure_search_key=os.getenv("SEARCH_API_KEY"),
    index_name=os.getenv("SEARCH_INDEX_NAME"),
    embedding_function=embeddings.embed_query
)
logging.info("AzureSearch vector store connected.")

# Helper Functions


def search(query: str):
    """Performs a similarity search in the vector store."""
    logging.info(f"Performing search for query: '{query}'")
    retrieved_docs = vector_store.similarity_search_with_relevance_scores(
        query=query,
        k=3,
    )

    if retrieved_docs:
        top_doc, score = retrieved_docs[0]
        logging.info(f"Top matching document found with score: {score:.4f}")
        return top_doc.page_content
    else:
        logging.warning("No documents found for the query.")
        return None


def assistant(user_query: str, context: str | None):
    """Generates a response using the LLM with the provided context."""
    if context is None:
        context = "No relevant wine information was found in the database."

    messages = [
        {"role": "system", "content": "You are a helpful wine assistant. Use the provided context to answer the user's question about wine. If the context is not relevant to the question, say that you couldn't find information in the database about that topic."},
        {"role": "user", "content": user_query},
        {"role": "assistant", "content": f"Context from wine database: {context}"}
    ]

    logging.info("Sending request to Azure OpenAI Chat completion.")
    response = llm_client.chat.completions.create(
        model=os.getenv("CHAT_DEPLOYMENT_NAME"),
        messages=messages,
    )

    final_response = response.choices[0].message.content

    return final_response


# API Endpoints
class Body(BaseModel):
    query: str


@app.get("/", include_in_schema=False)
def root():
    return RedirectResponse(url="/docs", status_code=301)


@app.post("/ask")
def ask(body: Body):
    """
    Takes a user query, searches for relevant context in the vector database,
    and returns a response generated by the language model.
    """
    context = search(body.query)
    chatbot_response = assistant(body.query, context)
    return {"response": chatbot_response, "retrieved_context": context}
